





from http.cookiejar import MISSING_FILENAME_TEXT

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt


df = pd.read_csv('table.csv') # read the data from Ghirlanda et al. 2008





def get_float(col):
    return np.array([float(base) * 10**float(exponent) for val in col for base, exponent in [str(val).split('e')]])





Epeak = np.log10(df['Epeak']) # keV
Eiso = np.log10(get_float(df['Eiso'])) # ergs

plt.figure()
plt.scatter(Eiso, Epeak)

plt.xlabel(r'$log(E_{\text{iso}})$')
plt.ylabel(r'$log(E_{\text{peak}})$')





def GoF(m,k, x=Eiso, y=Epeak):
    """
    Goodness of fit following the chi-squared formula
    :param m: slope
    :param k: y-axis cut
    :param x: x-values
    :param y: y-values
    :return: chi-squared value
    """
    model = m*x + k
    chi_squared = np.sum((y-model)**2)
    return chi_squared


# Define parameter ranges
k = np.linspace(-25.,-20.,100) # from the previous scatter plot, we can infer that the y-axis cut is around -25 and -20
m = np.linspace(0.0,1.0,100) # the slope must be positive

G = np.zeros([len(m), len(k)]) # G[i,j] corresponds to m[i], k[j]
for i in range(len(m)):
    for j in range(len(k)):
        G[i,j] = GoF(m[i], k[j], x=Eiso, y=Epeak)


# extract best fit parameters
best_fit = np.argmin(G)
m_index, k_index = np.unravel_index(best_fit, G.shape)
m_fit, k_fit = m[m_index], k[k_index]
print(m_fit, k_fit)

plt.figure()
plt.contourf(k, m, np.log10(G))#, levels=50)
plt.colorbar(label=r'$\log_{10} (\text{GoF})$')
plt.xlabel(r'Parameter $k$')
plt.ylabel(r'Parameter $m$')
plt.scatter(k_fit, m_fit, marker='1', color='r')





plt.figure()
plt.scatter(Eiso, Epeak)
plt.plot(Eiso, k_fit+Eiso*m_fit, label=f'k={k_fit}, m={m_fit}', color='r')
# plt.plot(Eiso, -24+Eiso*0.5, color='g', label=f'k={-24}, m={0.5}') # line used to calibrate the parameter space
plt.legend()
plt.xlabel(r'$E_\text{iso}$')
plt.ylabel(r'$E_\text{peak}$')


def measure_distances(a, b, x, y):
    """
    Measures the distance between a point p = (x,y) and a line of best fit y=ax+b.
    :param p: coordinates of the point
    :param a: slope of the fit
    :param b: y-axis cut of the fit
    :return: scalar for the distance
    """
    # for the line point we use x0=0 and thus y0=b
    numerator = abs(a*x -y +b)
    denominator = np.sqrt(1 + (a ** 2))
    return numerator / denominator

def gaussian(x, mu, sigma):
    return (1/sigma*np.sqrt(2*np.pi)) * np.exp(-(x-mu)**2/(2*sigma**2)) # gaussian function


plt.figure()
distances = np.array([measure_distances(m_fit, k_fit, Eiso_i, Epeak_i) for Eiso_i, Epeak_i in zip(Eiso, Epeak)])
plt.hist(distances)

median = np.sum(distances)/len(distances)
plt.vlines(median, 0, 14, color='red')




















sqchi = GoF(m_fit, k_fit, Eiso, Epeak)
print(sqchi)
sigma = sqchi/73
sigma = np.sqrt(sigma)
print(f'm = {m_fit:.4f} ± {sigma:.4f}')
print(f'k = {k_fit:.4f} ± {sigma:.4f}')





#Since the plotting of the $lg(\chi^2)$ surface was very weird with `plt.contourf` and it didn't coincide with the best fit parameters, I plotted a similar version manually as well as extracted the minimum value for `G`, obtaining the same fit results despite the weird plot. Additionally, I've performed the fit at the end with `scipy.curve_fit`, obtaining the exact same results.

for i in range(len(m)):
    for j in range(len(k)):
        val = np.log10(G[i][j])
        if 0<val and val<=.6:
            plt.scatter(m[i],k[j],color='blue', s=1,marker='s')
        elif .6<val and val<=1:
            plt.scatter(m[i],k[j],color='green', s=1,marker='s')
        elif 1<val and val<=2:
            plt.scatter(m[i],k[j],color='orange', s=1,marker='s')
        else:
            plt.scatter(m[i],k[j],color='red', s=1,marker='s', alpha=0.3)
plt.scatter(m_fit, k_fit, marker='1', color='yellow')

minimum = 100
coords = [0,0]
for i in range(len(k)):
    for j in range(len(m)):
        if np.log10(G[i][j])<minimum:
            minimum = np.log10(G[i][j])
            coords = [k[i],m[j]]

print(fr'Minimum: {minimum}\t{10**minimum}')
print(f'k_fit={coords[0]}\tm_fit={coords[1]}')





from scipy.optimize import curve_fit

def linear_model(x, k, m):
    return k + m * x

# Fit the model
popt, pcov = curve_fit(linear_model, Eiso, Epeak)
k_best, m_best = popt

# Get parameter uncertainties
param_errors = np.sqrt(np.diag(pcov))
print(f"Best fit: k = {k_best:.3f} ± {param_errors[0]:.3f}")
print(f"Best fit: m = {m_best:.3f} ± {param_errors[1]:.3f}")

correlation_matrix = pcov / np.outer(param_errors, param_errors)
print("Correlation matrix:")
print(correlation_matrix)



